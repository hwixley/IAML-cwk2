{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "##########################################################\n",
    "#  Python script template for Question 1 (IAML Level 10)\n",
    "#  Note that\n",
    "#  - You should not change the filename of this file, 'iaml01cw2_q1.py', which is the file name you should use when you submit your code for this question.\n",
    "#  - You should define the functions shown below in your code.\n",
    "#  - You can define function arguments (parameters) and returns (attributes) if necessary.\n",
    "#  - In case you define helper functions, do not define them here, but put them in a separate Python module file, \"iaml01cw2_my_helpers.py\", and import it in this script.\n",
    "#  - For those questions requiring you to show results in tables, your code does not need to present them in tables - just showing them with print() is fine.\n",
    "#  - You do not need to include this header in your submission\n",
    "##########################################################\n",
    "\n",
    "#--- Code for loading the data set and pre-processing --->\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../helpers'))\n",
    "from iaml01cw2_helpers import *\n",
    "\n",
    "# Load the data:\n",
    "dataPath = os.path.join(os.getcwd(),'../data')\n",
    "Xtrn, Ytrn, Xtst, Ytst = load_FashionMNIST(dataPath)\n",
    "Xtrn_orig = Xtrn.copy\n",
    "Xtst_orig = Xtst.copy\n",
    "Xtrn = Xtrn/255.0\n",
    "Xtst = Xtst/255.0\n",
    "\n",
    "Xmean = Xtrn.mean(0)\n",
    "Xtrn_nm = Xtrn - Xmean\n",
    "Xtst_nm = Xtst - Xmean\n",
    "#<----\n",
    "\n",
    "# Q1.1\n",
    "def iaml01cw2_q1_1():\n",
    "    \n",
    "    #Print elements of first training sample\n",
    "    print(\"First 4 elements of the first training sample in Xtrn_nm:\")\n",
    "    print(Xtrn_nm[0,0:4])\n",
    "    \n",
    "    #Print elements of last training sample\n",
    "    print(\"\\nFirst 4 elements of the last training sample in Xtrn_nm:\")\n",
    "    print(Xtrn_nm[-1,0:4])\n",
    "    \n",
    "#\n",
    "# iaml01cw2_q1_1()   # comment this out when you run the function\n",
    "\n",
    "# Q1.2\n",
    "def iaml01cw2_q1_2():\n",
    "    \n",
    "    print(\"Starting Q1.2...\\n\")\n",
    "    \n",
    "    # Array of classes\n",
    "    classes = np.unique(Ytrn)\n",
    "    \n",
    "    # Initialisation of variables\n",
    "    classSamples = []\n",
    "    means = []\n",
    "    closest = []\n",
    "    furthest = []\n",
    "    classMeans = np.zeros((10,784))\n",
    "    \n",
    "    # Initialise empty nested arrays\n",
    "    for c in classes:\n",
    "        classSamples.append([])\n",
    "        closest.append([])\n",
    "        furthest.append([])\n",
    "    \n",
    "    # Add all the indexes of samples for a given classs in the classSamples list\n",
    "    for row in range(Ytrn.size): \n",
    "        classSamples[Ytrn[row]].append(row)\n",
    "    \n",
    "    # Iterates over all classes (to calculate means, closest and furthest samples)\n",
    "    for c in range(10):\n",
    "        #Represents the relevant samples for the given class c\n",
    "        samples = classSamples[c]\n",
    "        \n",
    "        #Calculate the mean samples for each class\n",
    "        for index in samples:\n",
    "            classMeans[c] += Xtrn[index,:]\n",
    "            \n",
    "        classMeans[c] = classMeans[c]/len(samples)\n",
    "        meanC = classMeans[c].reshape(28,28)\n",
    "        means.append(meanC)\n",
    "        \n",
    "        #Calculate closest samples variables\n",
    "        close1 = -1\n",
    "        closeDist1 = 10000\n",
    "        close2 = -1\n",
    "        closeDist2 = 10000\n",
    "        #Calculate furthest samples variables\n",
    "        far1 = -1\n",
    "        farDist1 = 0\n",
    "        far2 = -1\n",
    "        farDist2 = 0\n",
    "        \n",
    "        #Iterate through the samples of the given class to find the closest & furthest samples\n",
    "        for index in samples:\n",
    "            dist = sum((classMeans[c]-Xtrn[index,:])**2)\n",
    "            \n",
    "            if dist < closeDist1:\n",
    "                close1 = index\n",
    "                closeDist1 = dist\n",
    "            elif dist < closeDist2:\n",
    "                close2 = index\n",
    "                closeDist2 = dist\n",
    "                \n",
    "            if dist > farDist1:\n",
    "                far1 = index\n",
    "                farDist1 = dist\n",
    "            elif dist > farDist2:\n",
    "                far2 = index\n",
    "                farDist2 = dist\n",
    "        \n",
    "        #Store the closest and furthest samples\n",
    "        closest[c].append(close1)\n",
    "        closest[c].append(close2)\n",
    "        furthest[c].append(far1)\n",
    "        furthest[c].append(far2)\n",
    "    \n",
    "    #Create a subplot to show the relevant samples\n",
    "    fig, axs = plt.subplots(10,5,figsize=(16,16))\n",
    "    fig.tight_layout(h_pad=2.5)\n",
    "    for i in range(10):\n",
    "        #Mean\n",
    "        axs[i,0].imshow(means[i], cmap=\"gray_r\")\n",
    "        axs[i,0].set_ylabel(\"Class \" + str(i) + \"\\n\",fontsize=20)\n",
    "        axs[i,0].set_xticklabels([])\n",
    "        axs[i,0].set_yticklabels([])\n",
    "        \n",
    "        for n in range(1,3):\n",
    "            #Closest\n",
    "            axs[i,n].imshow(Xtrn[closest[i][n-1],:].reshape(28,28), cmap=\"gray_r\")\n",
    "            axs[i,n].set_title(\"Sample \" + str(closest[i][n-1]),fontsize=18)\n",
    "            axs[i,n].set_xticklabels([])\n",
    "            axs[i,n].set_yticklabels([])\n",
    "            \n",
    "            #Furthest\n",
    "            axs[i,n+2].imshow(Xtrn[furthest[i][-n+2],:].reshape(28,28), cmap=\"gray_r\")\n",
    "            axs[i,n+2].set_title(\"Sample \" + str(furthest[i][-n+2]),fontsize=18)\n",
    "            axs[i,n+2].set_xticklabels([])\n",
    "            axs[i,n+2].set_yticklabels([])\n",
    "    \n",
    "    #Add appropriate axis labels to the subplot\n",
    "    axs[9,0].set_xlabel(\"\\nMean sample\",fontsize=20)\n",
    "    axs[9,1].set_xlabel(\"\\nClosest sample\\nto mean\",fontsize=20)\n",
    "    axs[9,2].set_xlabel(\"\\n2nd closest sample\\nto mean\",fontsize=20)\n",
    "    axs[9,3].set_xlabel(\"\\n2nd furthest sample\\nfrom mean\",fontsize=20)\n",
    "    axs[9,4].set_xlabel(\"\\nFurthest sample\\nfrom mean\",fontsize=20)\n",
    "            \n",
    "                \n",
    "#\n",
    "# iaml01cw2_q1_2()   # comment this out when you run the function\n",
    "\n",
    "# Q1.3\n",
    "def iaml01cw2_q1_3():\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    print(\"Starting Q1.3...\\n\")\n",
    "    \n",
    "    #Apply PCA to our normalized training data\n",
    "    pca = PCA().fit(Xtrn_nm)\n",
    "    \n",
    "    #Print the explained variance for the first 5 PCs\n",
    "    print(\"The explained variances for the first 5 principal components:\")\n",
    "    \n",
    "    for i in range(5):\n",
    "        print(\"PC \" + str(i+1) + \" = \" + str(pca.explained_variance_[i]))\n",
    "    \n",
    "    \n",
    "#\n",
    "# iaml01cw2_q1_3()   # comment this out when you run the function\n",
    "\n",
    "\n",
    "# Q1.4\n",
    "def iaml01cw2_q1_4():\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "    \n",
    "    print(\"Starting Q1.4...\\n\")\n",
    "    \n",
    "    #Initialize arrays to store number of attributes and cumVar\n",
    "    y = np.empty(784)\n",
    "    x = np.arange(1,785)\n",
    "    \n",
    "    #Iterate through all different possible number of PCA components\n",
    "    for n in range(784):\n",
    "        pca = PCA(n_components=n+1).fit(Xtrn_nm)\n",
    "        y[n] = pca.explained_variance_ratio_.sum()\n",
    "    \n",
    "    #Plot the data appropriately\n",
    "    plt.plot(x,y)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(\"K\",fontsize=16)\n",
    "    plt.ylabel(\"Cumulative explained variance ratio\",fontsize=16)\n",
    "    plt.title(\"A graph to show the relationship between the cumulative explained variance\\nratio and the number of principal components (K) for 'Xtrn_nm'\",fontsize=18)\n",
    "\n",
    "#\n",
    "# iaml01cw2_q1_4()   # comment this out when you run the function\n",
    "\n",
    "\n",
    "# Q1.5\n",
    "def iaml01cw2_q1_5():\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    print(\"Starting Q1.5...\\n\")\n",
    "    \n",
    "    #Initialize PCA model\n",
    "    pca = PCA().fit(Xtrn_nm)\n",
    "    \n",
    "    #Plot the first 10 principal components in a 2-by-5 grid\n",
    "    fig, axs = plt.subplots(2,5,figsize=(15,15))\n",
    "    index = 0\n",
    "    for r in range(2):\n",
    "        for c in range(5):\n",
    "            axs[r,c].imshow(pca.components_[index].reshape(28,28))\n",
    "            axs[r,c].set_title(\"PC \" + str(index+1),fontsize=18)\n",
    "            index += 1\n",
    "\n",
    "#\n",
    "# iaml01cw2_q1_5()   # comment this out when you run the function\n",
    "\n",
    "\n",
    "# Q1.6\n",
    "def iaml01cw2_q1_6():\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from math import sqrt\n",
    "    \n",
    "    print(\"Starting Q1.6...\\n\")\n",
    "    \n",
    "    #Set up arrays to store relevant data\n",
    "    classes = np.unique(Ytrn)\n",
    "    K = [5,20,50,200]\n",
    "    orig_classSamples = np.empty((10,784))\n",
    "    \n",
    "    # Get indexes of the first samples for each class\n",
    "    for c in classes:\n",
    "        for s in range(Ytrn.size):\n",
    "            if Ytrn[s] == c:\n",
    "                orig_classSamples[c,:] = Xtrn_nm[s,:]\n",
    "                break\n",
    "    \n",
    "    #Numpy matrix to store all the RMSE scores\n",
    "    rmse = np.empty((10,4))\n",
    "    \n",
    "    #Iterate through the different values of K\n",
    "    for i in range(4):\n",
    "        \n",
    "        #Initialize PCA model\n",
    "        pca = PCA(n_components=K[i]).fit(Xtrn_nm)\n",
    "        \n",
    "        #Iterate through the different classes\n",
    "        for s in range(10):\n",
    "            \n",
    "            #Original X vector\n",
    "            X = orig_classSamples[s,:]\n",
    "            \n",
    "            #Transformed X vector\n",
    "            newX = pca.transform(X.reshape(1,-1))\n",
    "            \n",
    "            #Reconstructed X vector\n",
    "            inverseX = pca.inverse_transform(newX.reshape(-1))\n",
    "            \n",
    "            #Output RMSE to rmse numpy array\n",
    "            rmse[s,i] = sqrt(mean_squared_error(X,inverseX))\n",
    "    \n",
    "    #Print the rounded RMSE array \n",
    "    print(np.round(rmse,3))\n",
    "            \n",
    "            \n",
    "#\n",
    "# iaml01cw2_q1_6()   # comment this out when you run the function\n",
    "\n",
    "\n",
    "# Q1.7\n",
    "def iaml01cw2_q1_7():\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    print(\"Starting Q1.7...\\n\")\n",
    "    \n",
    "    #Set up arrays to store relevant data\n",
    "    classes = np.unique(Ytrn)\n",
    "    K = [5,20,50,200]\n",
    "    orig_classSamples = np.empty((10,784))\n",
    "    \n",
    "    # Get the first samples for each class\n",
    "    for c in classes:\n",
    "        for s in range(Ytrn.size):\n",
    "            if Ytrn[s] == c:\n",
    "                orig_classSamples[c,:] = Xtrn_nm[s,:]\n",
    "                break\n",
    "    \n",
    "    #Variables to store the inversely transformed samples\n",
    "    index1 = 0\n",
    "    new_classSamples = np.empty((40,784))\n",
    "    \n",
    "    #Iterate through the values of k\n",
    "    for i in range(4):\n",
    "        k = K[i]\n",
    "        print(i)\n",
    "        \n",
    "        #Iterate through the classes\n",
    "        for s in range(10):\n",
    "            pca = PCA(n_components=k,random_state=1000)\n",
    "            pca.fit(Xtrn_nm)\n",
    "            X = orig_classSamples[s,:]\n",
    "            newX = pca.transform(X.reshape(1,-1))\n",
    "            \n",
    "            inverseX = pca.inverse_transform(newX).reshape(-1,1)[:,0]\n",
    "            new_classSamples[index1,:] = inverseX\n",
    "            index1 += 1\n",
    "    \n",
    "    #Put all of these images into a subplot\n",
    "    fig, axs = plt.subplots(10,4,figsize=(18,18))\n",
    "    index = 0\n",
    "    for k in range(4):\n",
    "        for c in range(10):\n",
    "            if k == 0:\n",
    "                axs[c,k].set_ylabel(\"Class \" + str(c),fontsize=20)\n",
    "                \n",
    "            axs[c,k].imshow(new_classSamples[index,:].reshape(28,28))\n",
    "            axs[c,k].set_xticklabels([])\n",
    "            axs[c,k].set_yticklabels([])\n",
    "            index += 1\n",
    "            \n",
    "    axs[9,0].set_xlabel(\"K = 5\",fontsize=20)\n",
    "    axs[9,1].set_xlabel(\"K = 20\",fontsize=20)\n",
    "    axs[9,2].set_xlabel(\"K = 50\",fontsize=20)\n",
    "    axs[9,3].set_xlabel(\"K = 2000\",fontsize=20)\n",
    "    \n",
    "    plt.show()\n",
    "#\n",
    "# iaml01cw2_q1_7()   # comment this out when you run the function\n",
    "\n",
    "\n",
    "# Q1.8\n",
    "def iaml01cw2_q1_8():\n",
    "    from sklearn.decomposition import PCA\n",
    "    import matplotlib as m\n",
    "    import matplotlib.cm as cm\n",
    "    from matplotlib.cm import ScalarMappable\n",
    "    from matplotlib.colors import Normalize\n",
    "    \n",
    "    print(\"Starting Q1.8...\\n\")\n",
    "    \n",
    "    pca = PCA(n_components=2,random_state=1000)\n",
    "    pca.fit(Xtrn_nm)\n",
    "    newXtrn = pca.transform(Xtrn_nm)\n",
    "    \n",
    "    x = newXtrn[:,0]\n",
    "    y = newXtrn[:,1]\n",
    "    z = np.empty((60000,4))#.reshape(-1,1)\n",
    "    classes = np.unique(Ytrn)\n",
    "    coolwarm = cm.get_cmap(\"coolwarm\",10)\n",
    "        \n",
    "    #print(plt.get_cmap(\"coolwarm\").count)\n",
    "    for s in range(Ytrn.size):\n",
    "        c = coolwarm(Ytrn[s]*(1/9))\n",
    "        #print(len(c))\n",
    "        z[s,0] = c[0]\n",
    "        z[s,1] = c[1]\n",
    "        z[s,2] = c[2]\n",
    "        z[s,3] = c[3]\n",
    "    \n",
    "    plt.scatter(x,y,c=z,cmap=\"coolwarm\",alpha = 0.5)\n",
    "    plt.show()\n",
    "#\n",
    "# iaml01cw2_q1_8()   # comment this out when you run the function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Q1.6...\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[-3.13725490e-06 -2.26797386e-05 -1.17973856e-04 -4.07058824e-04\n -9.79150327e-04  2.29522876e-03 -3.22222222e-03 -8.72444444e-03\n -2.23477124e-02 -5.66064706e-02  3.15452941e-02  5.57182092e-01\n  1.99636863e-01  1.22722222e-02 -9.89359477e-03 -4.27702614e-02\n  1.39108693e-01  4.99398954e-01  4.19139739e-01  2.19269935e-02\n -1.61122876e-02 -7.76346405e-03 -4.63261438e-03 -3.33450980e-03\n -1.87000000e-03 -1.02065359e-03 -3.82941176e-04 -6.59477124e-05\n -2.79084967e-05 -9.43137255e-05 -3.95620915e-04  2.49437908e-03\n -3.01934641e-03 -8.17150327e-03 -2.88511111e-02  1.23387843e-01\n  4.07664902e-01  6.51259020e-01  5.34504379e-01  5.01346601e-01\n  4.82032876e-01  5.63772157e-01  5.72227712e-01  5.50213987e-01\n  5.66335621e-01  4.93520719e-01  5.61864248e-01  8.19084052e-01\n  8.03263203e-01  5.73993856e-01  1.52184510e-01 -1.36563399e-02\n -8.66692810e-03 -4.73875817e-03 -1.83477124e-03 -3.27254902e-04\n -1.02483660e-04 -3.18235294e-04 -1.04516340e-03 -2.84457516e-03\n -8.26013072e-03  1.76982353e-02  5.99788366e-01  7.33696667e-01\n  6.83729739e-01  5.51502614e-01  4.27903268e-01  3.79244052e-01\n  3.55256340e-01  3.64815556e-01  4.22724444e-01  3.75195033e-01\n  3.80181176e-01  3.94274706e-01  4.36625490e-01  5.32396275e-01\n  6.41666340e-01  7.26370523e-01  8.02339935e-01  6.08877582e-01\n -1.86156863e-02 -1.03868627e-02 -4.44967320e-03 -8.84705882e-04\n -2.37385621e-04 -8.98366013e-04 -2.21418301e-03 -5.62431373e-03\n -2.35283660e-02  6.60559673e-01  7.22287190e-01  5.90496601e-01\n  5.21601242e-01  4.61854379e-01  3.87480784e-01  3.79904641e-01\n  3.70863399e-01  3.73949085e-01  3.50566863e-01  3.56171373e-01\n  3.58857516e-01  3.73679150e-01  3.89403856e-01  4.57892484e-01\n  5.34931111e-01  6.00825686e-01  6.55248954e-01  8.12689935e-01\n  1.26716993e-01 -1.83478431e-02 -8.44803922e-03 -1.89823529e-03\n -4.70261438e-04 -2.04313725e-03 -4.52411765e-03 -1.18547059e-02\n  1.54781830e-01  7.47213987e-01  5.95570654e-01  5.58280392e-01\n  5.12559673e-01  4.82570000e-01  5.37862026e-01  5.71421961e-01\n  5.72529739e-01  5.45619412e-01  5.52739935e-01  5.36535359e-01\n  5.38888758e-01  5.62676993e-01  5.58259935e-01  5.58607059e-01\n  5.14186993e-01  5.49171373e-01  6.15427255e-01  7.62613791e-01\n  4.91721111e-01 -3.13664706e-02 -1.54506536e-02 -3.54267974e-03\n -1.08398693e-03 -4.29803922e-03 -8.78006536e-03 -2.19075817e-02\n  3.87055425e-01  7.39314967e-01  5.89039346e-01  5.37671307e-01\n  5.07894967e-01  4.34688889e-01 -5.82352941e-03 -1.09834641e-01\n -2.23200000e-01 -1.32511438e-01 -2.44552157e-01 -2.42900196e-01\n -2.68913725e-01 -2.46926405e-01 -1.90574118e-01  4.66033987e-02\n  5.36784444e-01  5.02941569e-01  5.77510654e-01  7.13297843e-01\n  7.90106405e-01 -5.21483660e-02 -2.70550327e-02 -5.92751634e-03\n -1.85000000e-03 -7.04228758e-03 -1.42435294e-02 -3.48233987e-02\n  6.94328039e-01  6.95606209e-01  5.76374967e-01  5.12118431e-01\n  5.24572222e-01  4.10508824e-01 -4.04996797e-01 -1.34470784e-01\n -2.87418301e-02 -4.27690654e-01 -4.15183007e-02 -2.25888301e-01\n -2.00836732e-01 -1.91666601e-01 -2.66911111e-01 -4.23991307e-01\n  4.92462092e-01  4.81003137e-01  5.66864248e-01  6.34871895e-01\n  8.59769542e-01  7.10339869e-02 -3.99370588e-02 -8.45960784e-03\n -3.00111111e-03 -1.06699346e-02 -2.05615686e-02 -4.75401961e-02\n  8.59895294e-01  6.57736078e-01  5.68783203e-01  5.20059216e-01\n  5.12293987e-01  4.62098497e-01 -4.08759085e-01  5.80115163e-01\n  5.31900980e-01  1.08814641e-01  5.45263660e-01  2.02563529e-01\n  4.91119477e-01  5.04982288e-01  3.24531569e-01 -4.47653660e-01\n  4.44388497e-01  4.88518235e-01  5.29989412e-01  5.90397582e-01\n  7.10312810e-01  2.67278824e-01 -5.30282353e-02 -1.07608497e-02\n -3.93339869e-03 -1.40507843e-02 -2.70186928e-02  1.18001830e-01\n  7.63489020e-01  6.25519542e-01  5.63938170e-01  5.22787582e-01\n  5.35307255e-01  4.24216144e-01 -4.13082549e-01 -2.58309150e-01\n -2.94769085e-01 -2.11612876e-01 -3.94193137e-01 -4.41778301e-01\n -4.78371569e-01 -4.74748105e-01 -2.24865686e-01 -4.67426340e-01\n  3.09907908e-01  4.50807908e-01  4.87955817e-01  5.46961569e-01\n  6.47779869e-01  4.77988562e-01 -6.85565359e-02 -1.39813725e-02\n -4.78529412e-03 -1.77064706e-02 -3.18633333e-02  5.77232157e-01\n  7.88232026e-01  6.13163007e-01  5.43813268e-01  5.32607320e-01\n  4.94172876e-01  4.05742745e-01 -2.16587516e-01 -1.55362680e-01\n -9.81077778e-02 -9.66064706e-02 -1.49729346e-01 -2.44843856e-01\n -2.92122157e-01 -2.62658170e-01 -1.61644510e-01 -1.40263268e-01\n  3.53274706e-01  3.93502941e-01  5.01846797e-01  5.44870392e-01\n  6.44815425e-01  6.49529542e-01 -8.77126797e-02 -1.82133333e-02\n -6.01895425e-03 -2.12384314e-02 -3.45835948e-02  3.47612353e-01\n  5.95302222e-01  6.43252810e-01  6.45425294e-01  6.41855163e-01\n  4.70695752e-01  3.90381373e-01  5.54847190e-01  5.28155882e-01\n  4.94374379e-01  4.61496013e-01  4.63775359e-01  4.04067451e-01\n  4.02833137e-01  3.95424314e-01  4.40583072e-01  4.42836928e-01\n  3.20409412e-01  4.00686667e-01  4.95774837e-01  5.28447255e-01\n  5.18067908e-01  2.46323791e-01 -1.13002876e-01 -2.26980392e-02\n -7.47071895e-03 -2.43664052e-02 -3.57705229e-02 -6.66712418e-02\n -1.35845882e-01 -1.61558562e-01 -6.66619608e-02  2.96404706e-01\n  5.09145294e-01  3.58943660e-01  3.54363203e-01  2.63116078e-01\n  2.50546993e-01  2.56698889e-01  2.58960915e-01  1.89410523e-01\n  1.85176405e-01  1.89855033e-01  2.25680719e-01  2.49816797e-01\n  3.37444444e-01  2.01833529e-01 -4.03112418e-01 -3.34374575e-01\n -2.54866732e-01 -2.16801111e-01 -1.39963595e-01 -2.75905882e-02\n -9.05124183e-03 -2.71558170e-02 -3.80196078e-02 -6.85377778e-02\n -1.38144444e-01 -2.27207516e-01 -2.77310523e-01 -1.45072614e-01\n  5.05557255e-01  3.20534314e-01  3.35026209e-01  2.71597451e-01\n  2.59995490e-01  2.58314967e-01  2.63989216e-01  2.02601176e-01\n  2.00219608e-01  1.92552484e-01  2.43280523e-01  2.40419020e-01\n  3.48805098e-01 -4.44836601e-02 -4.21184444e-01 -3.53188758e-01\n -2.67184314e-01 -2.27107255e-01 -1.63295686e-01 -3.34662092e-02\n -1.05666013e-02 -3.08207190e-02 -4.27741830e-02 -7.57775817e-02\n -1.47275948e-01 -2.20213791e-01 -2.97562157e-01 -1.70309804e-01\n  4.29908105e-01  3.02708954e-01  3.06438039e-01  2.47993399e-01\n  2.39493268e-01  2.48037255e-01  2.56803333e-01  1.99978954e-01\n  1.94438758e-01  1.89094967e-01  2.35103791e-01  2.50846078e-01\n  3.49099869e-01 -1.14520523e-01 -4.30792745e-01 -3.52523268e-01\n -2.70540000e-01 -2.31323333e-01 -1.81573007e-01 -4.10290196e-02\n -1.43754902e-02 -4.46743137e-02 -6.06222876e-02 -9.89322222e-02\n -1.83121569e-01 -2.74882876e-01 -3.27210654e-01 -3.25081111e-01\n  3.61984379e-01  2.99109412e-01  2.59543007e-01  2.23195686e-01\n  2.18030850e-01  2.44897386e-01  2.54273725e-01  2.00433791e-01\n  1.93513791e-01  1.84843529e-01  2.00233137e-01  2.71945163e-01\n  3.59721895e-01 -1.66873725e-01 -4.31493791e-01 -3.49714902e-01\n -2.79317255e-01 -2.34056405e-01 -1.96158693e-01 -4.91878431e-02\n -2.46235948e-02 -7.24652941e-02 -8.84316993e-02 -1.28219542e-01\n -2.15785359e-01 -2.92484183e-01 -3.44707255e-01 -3.68765752e-01\n  3.26437516e-01  2.68884706e-01  2.18872222e-01  2.11430915e-01\n  2.08644967e-01  2.33784314e-01  2.53112614e-01  2.01764444e-01\n  1.94200588e-01  1.85115098e-01  1.67171961e-01  2.81127582e-01\n  3.67604641e-01 -1.80215490e-01 -4.34704510e-01 -3.56113137e-01\n -2.88669608e-01 -2.39979281e-01 -2.09131046e-01 -5.71667320e-02\n -3.58619608e-02 -9.16413725e-02 -1.07023725e-01 -1.49787974e-01\n -2.39781895e-01 -3.14228824e-01 -3.63369608e-01 -3.91670458e-01\n  2.95196667e-01  2.37669608e-01  1.92850719e-01  2.01652810e-01\n  2.01903464e-01  2.21827582e-01  2.51938235e-01  2.01196797e-01\n  1.88720065e-01  1.85584837e-01  1.58926536e-01  2.68860065e-01\n  3.83785294e-01 -1.82370458e-01 -4.32347582e-01 -3.62802092e-01\n -2.94101961e-01 -2.41686732e-01 -2.12249542e-01 -6.00363399e-02\n -3.69159477e-02 -1.00243660e-01 -1.20796863e-01 -1.66643464e-01\n -2.58403268e-01 -3.27721176e-01 -3.76694510e-01 -4.00078889e-01\n  3.87327778e-01  2.26111765e-01  1.94358627e-01  1.90632353e-01\n  2.01266863e-01  2.25798431e-01  2.51359542e-01  2.05307582e-01\n  1.90653399e-01  1.91196405e-01  1.80432680e-01  2.64604902e-01\n  4.13391176e-01 -1.48756928e-01 -4.15798431e-01 -3.51479542e-01\n -2.85324510e-01 -2.29417451e-01 -1.99716732e-01 -5.63823529e-02\n -3.01676471e-02 -9.76177778e-02 -1.26256471e-01 -1.73636667e-01\n -2.65089281e-01 -3.33428758e-01 -3.78185425e-01 -3.97559216e-01\n  3.72144967e-01  2.27018627e-01  2.23096209e-01  1.97598562e-01\n  2.14036536e-01  2.43105425e-01  2.68208824e-01  2.22294379e-01\n  2.10643137e-01  2.01314379e-01  2.23202614e-01  2.70332745e-01\n  4.34901307e-01 -1.07095490e-01 -3.99822092e-01 -3.42500000e-01\n -2.74363725e-01 -2.16654118e-01 -1.85544183e-01 -5.21488235e-02\n -2.69092810e-02 -8.94477778e-02 -1.19334837e-01 -1.69826405e-01\n -2.61688889e-01 -3.26196993e-01 -3.69784248e-01 -3.86404706e-01\n  3.46466993e-01  2.51082484e-01  2.20660915e-01  2.17827451e-01\n  2.17043725e-01  2.50315556e-01  2.92166340e-01  2.29744183e-01\n  2.21163137e-01  2.33466732e-01  2.60312484e-01  2.76699542e-01\n  4.56827516e-01 -2.29432680e-02 -3.83825621e-01 -3.28324248e-01\n -2.61531895e-01 -2.02282288e-01 -1.64489608e-01 -4.21787582e-02\n -2.60863399e-02 -8.41080392e-02 -1.11604575e-01 -1.60746013e-01\n -2.51458235e-01 -3.14636078e-01 -3.56500196e-01 -3.70221176e-01\n  3.50043268e-01  2.67689281e-01  2.37881176e-01  2.38430588e-01\n  2.34688105e-01  2.68466536e-01  3.21547190e-01  2.59031111e-01\n  2.50132941e-01  2.61280196e-01  3.00850327e-01  3.04639216e-01\n  4.73766928e-01  4.59514379e-02 -3.56900850e-01 -3.06760000e-01\n -2.36929869e-01 -1.75122026e-01 -1.32750588e-01 -3.23790850e-02\n -2.31898693e-02 -7.47670588e-02 -1.01447843e-01 -1.50614052e-01\n -2.39469608e-01 -3.00198235e-01 -3.37648954e-01 -3.46838889e-01\n  3.75038431e-01  3.02212745e-01  2.61175033e-01  2.55510327e-01\n  2.57421373e-01  2.90548170e-01  3.54059869e-01  2.93592157e-01\n  2.83651830e-01  2.94339020e-01  3.45183529e-01  3.35472810e-01\n  4.91293007e-01  1.19536732e-01 -3.28586275e-01 -2.79588954e-01\n -2.11469542e-01 -1.50072680e-01 -1.10000327e-01 -2.71755556e-02\n -1.86118301e-02 -5.95922222e-02 -8.45809150e-02 -1.32571176e-01\n -2.19082549e-01 -2.81392418e-01 -3.12722941e-01 -3.19213791e-01\n  3.97846144e-01  3.41043791e-01  2.96545294e-01  2.80494444e-01\n  2.79238235e-01  3.20471438e-01  3.84243660e-01  3.41609346e-01\n  3.22090784e-01  3.28669935e-01  3.87359085e-01  3.74165621e-01\n  5.09932680e-01  1.64979608e-01 -2.98667320e-01 -2.50876144e-01\n -1.85425163e-01 -1.24582941e-01 -8.61005229e-02 -1.98208497e-02\n -1.30311765e-02 -4.33498039e-02 -6.69809150e-02 -1.13398301e-01\n -1.96454314e-01 -2.52097908e-01 -2.82717778e-01 -2.86636928e-01\n  4.07001765e-01  3.80811242e-01  3.36434510e-01  3.13555752e-01\n  3.04495098e-01  3.57398889e-01  4.06916013e-01  3.87872026e-01\n  3.60255033e-01  3.63122810e-01  4.17370784e-01  4.08413922e-01\n  5.29611765e-01  2.11408627e-01 -2.68002092e-01 -2.25563922e-01\n -1.58563203e-01 -9.89819608e-02 -6.29984314e-02 -1.36248366e-02\n -7.54366013e-03 -2.73781046e-02 -4.89088235e-02 -9.30378431e-02\n -1.72615556e-01 -2.26343137e-01 -2.51988301e-01 -2.52281307e-01\n  4.44940196e-01  4.29477320e-01  3.73549346e-01  3.51204837e-01\n  3.49817582e-01  4.01586797e-01  4.49511503e-01  4.27505098e-01\n  4.01442157e-01  3.96181961e-01  4.46832614e-01  4.45798235e-01\n  5.60676405e-01  2.54575425e-01 -2.40820654e-01 -2.00369673e-01\n -1.34548105e-01 -7.66333987e-02 -4.41557516e-02 -9.08588235e-03\n -3.69496732e-03 -1.55392157e-02 -3.33176471e-02 -7.27495425e-02\n -1.48236993e-01 -2.04694575e-01 -2.21163595e-01 -2.14657778e-01\n  3.88921438e-01  4.78455490e-01  4.09801046e-01  3.74011373e-01\n  3.71610196e-01  4.28995817e-01  4.71634967e-01  4.46201569e-01\n  4.17178170e-01  4.12340392e-01  4.65733660e-01  4.84338954e-01\n  5.91314379e-01  2.96797059e-01 -2.13524771e-01 -1.77157255e-01\n -1.08426928e-01 -5.40938562e-02 -2.76771242e-02 -5.28673203e-03\n -1.08993464e-03 -6.16823529e-03 -1.87308497e-02 -5.39100000e-02\n -1.24378039e-01 -1.78775948e-01 -1.92353203e-01 -1.80572876e-01\n  5.03485163e-01  5.66791438e-01  4.86473791e-01  5.31677908e-01\n  5.43176928e-01  5.88683333e-01  6.43846797e-01  6.22315229e-01\n  5.94163529e-01  5.86011503e-01  6.42490458e-01  5.94793595e-01\n  6.99316797e-01  4.62447582e-01 -1.87552092e-01 -1.43998889e-01\n -8.71020261e-02 -3.57489542e-02 -1.48366667e-02 -2.38633987e-03\n -9.15032680e-05 -8.33986928e-04 -3.89156863e-03 -1.70356863e-02\n -4.91890196e-02 -7.74798693e-02 -8.24131373e-02 -6.70140523e-02\n  4.58697647e-01  4.53124902e-01  3.46140850e-01  3.27241634e-01\n  3.23064771e-01  3.54958889e-01  3.22919020e-01  3.08977320e-01\n  2.93675033e-01  2.92266144e-01  3.31120131e-01  3.56044575e-01\n  4.44799020e-01  2.28109935e-01 -8.98778431e-02 -7.02623529e-02\n -3.32773203e-02 -1.06130719e-02 -3.21176471e-03 -2.77973856e-04].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-e88ecf2b3156>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0miaml01cw2_q1_6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-70-ecd891d00624>\u001b[0m in \u001b[0;36miaml01cw2_q1_6\u001b[0;34m()\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrn_nm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morig_classSamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0mnewX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0minverseX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/packages/miniconda3/envs/py3iaml/lib/python3.7/site-packages/sklearn/decomposition/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'mean_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'components_'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/packages/miniconda3/envs/py3iaml/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    442\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;31m# To ensure that array flags are maintained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[-3.13725490e-06 -2.26797386e-05 -1.17973856e-04 -4.07058824e-04\n -9.79150327e-04  2.29522876e-03 -3.22222222e-03 -8.72444444e-03\n -2.23477124e-02 -5.66064706e-02  3.15452941e-02  5.57182092e-01\n  1.99636863e-01  1.22722222e-02 -9.89359477e-03 -4.27702614e-02\n  1.39108693e-01  4.99398954e-01  4.19139739e-01  2.19269935e-02\n -1.61122876e-02 -7.76346405e-03 -4.63261438e-03 -3.33450980e-03\n -1.87000000e-03 -1.02065359e-03 -3.82941176e-04 -6.59477124e-05\n -2.79084967e-05 -9.43137255e-05 -3.95620915e-04  2.49437908e-03\n -3.01934641e-03 -8.17150327e-03 -2.88511111e-02  1.23387843e-01\n  4.07664902e-01  6.51259020e-01  5.34504379e-01  5.01346601e-01\n  4.82032876e-01  5.63772157e-01  5.72227712e-01  5.50213987e-01\n  5.66335621e-01  4.93520719e-01  5.61864248e-01  8.19084052e-01\n  8.03263203e-01  5.73993856e-01  1.52184510e-01 -1.36563399e-02\n -8.66692810e-03 -4.73875817e-03 -1.83477124e-03 -3.27254902e-04\n -1.02483660e-04 -3.18235294e-04 -1.04516340e-03 -2.84457516e-03\n -8.26013072e-03  1.76982353e-02  5.99788366e-01  7.33696667e-01\n  6.83729739e-01  5.51502614e-01  4.27903268e-01  3.79244052e-01\n  3.55256340e-01  3.64815556e-01  4.22724444e-01  3.75195033e-01\n  3.80181176e-01  3.94274706e-01  4.36625490e-01  5.32396275e-01\n  6.41666340e-01  7.26370523e-01  8.02339935e-01  6.08877582e-01\n -1.86156863e-02 -1.03868627e-02 -4.44967320e-03 -8.84705882e-04\n -2.37385621e-04 -8.98366013e-04 -2.21418301e-03 -5.62431373e-03\n -2.35283660e-02  6.60559673e-01  7.22287190e-01  5.90496601e-01\n  5.21601242e-01  4.61854379e-01  3.87480784e-01  3.79904641e-01\n  3.70863399e-01  3.73949085e-01  3.50566863e-01  3.56171373e-01\n  3.58857516e-01  3.73679150e-01  3.89403856e-01  4.57892484e-01\n  5.34931111e-01  6.00825686e-01  6.55248954e-01  8.12689935e-01\n  1.26716993e-01 -1.83478431e-02 -8.44803922e-03 -1.89823529e-03\n -4.70261438e-04 -2.04313725e-03 -4.52411765e-03 -1.18547059e-02\n  1.54781830e-01  7.47213987e-01  5.95570654e-01  5.58280392e-01\n  5.12559673e-01  4.82570000e-01  5.37862026e-01  5.71421961e-01\n  5.72529739e-01  5.45619412e-01  5.52739935e-01  5.36535359e-01\n  5.38888758e-01  5.62676993e-01  5.58259935e-01  5.58607059e-01\n  5.14186993e-01  5.49171373e-01  6.15427255e-01  7.62613791e-01\n  4.91721111e-01 -3.13664706e-02 -1.54506536e-02 -3.54267974e-03\n -1.08398693e-03 -4.29803922e-03 -8.78006536e-03 -2.19075817e-02\n  3.87055425e-01  7.39314967e-01  5.89039346e-01  5.37671307e-01\n  5.07894967e-01  4.34688889e-01 -5.82352941e-03 -1.09834641e-01\n -2.23200000e-01 -1.32511438e-01 -2.44552157e-01 -2.42900196e-01\n -2.68913725e-01 -2.46926405e-01 -1.90574118e-01  4.66033987e-02\n  5.36784444e-01  5.02941569e-01  5.77510654e-01  7.13297843e-01\n  7.90106405e-01 -5.21483660e-02 -2.70550327e-02 -5.92751634e-03\n -1.85000000e-03 -7.04228758e-03 -1.42435294e-02 -3.48233987e-02\n  6.94328039e-01  6.95606209e-01  5.76374967e-01  5.12118431e-01\n  5.24572222e-01  4.10508824e-01 -4.04996797e-01 -1.34470784e-01\n -2.87418301e-02 -4.27690654e-01 -4.15183007e-02 -2.25888301e-01\n -2.00836732e-01 -1.91666601e-01 -2.66911111e-01 -4.23991307e-01\n  4.92462092e-01  4.81003137e-01  5.66864248e-01  6.34871895e-01\n  8.59769542e-01  7.10339869e-02 -3.99370588e-02 -8.45960784e-03\n -3.00111111e-03 -1.06699346e-02 -2.05615686e-02 -4.75401961e-02\n  8.59895294e-01  6.57736078e-01  5.68783203e-01  5.20059216e-01\n  5.12293987e-01  4.62098497e-01 -4.08759085e-01  5.80115163e-01\n  5.31900980e-01  1.08814641e-01  5.45263660e-01  2.02563529e-01\n  4.91119477e-01  5.04982288e-01  3.24531569e-01 -4.47653660e-01\n  4.44388497e-01  4.88518235e-01  5.29989412e-01  5.90397582e-01\n  7.10312810e-01  2.67278824e-01 -5.30282353e-02 -1.07608497e-02\n -3.93339869e-03 -1.40507843e-02 -2.70186928e-02  1.18001830e-01\n  7.63489020e-01  6.25519542e-01  5.63938170e-01  5.22787582e-01\n  5.35307255e-01  4.24216144e-01 -4.13082549e-01 -2.58309150e-01\n -2.94769085e-01 -2.11612876e-01 -3.94193137e-01 -4.41778301e-01\n -4.78371569e-01 -4.74748105e-01 -2.24865686e-01 -4.67426340e-01\n  3.09907908e-01  4.50807908e-01  4.87955817e-01  5.46961569e-01\n  6.47779869e-01  4.77988562e-01 -6.85565359e-02 -1.39813725e-02\n -4.78529412e-03 -1.77064706e-02 -3.18633333e-02  5.77232157e-01\n  7.88232026e-01  6.13163007e-01  5.43813268e-01  5.32607320e-01\n  4.94172876e-01  4.05742745e-01 -2.16587516e-01 -1.55362680e-01\n -9.81077778e-02 -9.66064706e-02 -1.49729346e-01 -2.44843856e-01\n -2.92122157e-01 -2.62658170e-01 -1.61644510e-01 -1.40263268e-01\n  3.53274706e-01  3.93502941e-01  5.01846797e-01  5.44870392e-01\n  6.44815425e-01  6.49529542e-01 -8.77126797e-02 -1.82133333e-02\n -6.01895425e-03 -2.12384314e-02 -3.45835948e-02  3.47612353e-01\n  5.95302222e-01  6.43252810e-01  6.45425294e-01  6.41855163e-01\n  4.70695752e-01  3.90381373e-01  5.54847190e-01  5.28155882e-01\n  4.94374379e-01  4.61496013e-01  4.63775359e-01  4.04067451e-01\n  4.02833137e-01  3.95424314e-01  4.40583072e-01  4.42836928e-01\n  3.20409412e-01  4.00686667e-01  4.95774837e-01  5.28447255e-01\n  5.18067908e-01  2.46323791e-01 -1.13002876e-01 -2.26980392e-02\n -7.47071895e-03 -2.43664052e-02 -3.57705229e-02 -6.66712418e-02\n -1.35845882e-01 -1.61558562e-01 -6.66619608e-02  2.96404706e-01\n  5.09145294e-01  3.58943660e-01  3.54363203e-01  2.63116078e-01\n  2.50546993e-01  2.56698889e-01  2.58960915e-01  1.89410523e-01\n  1.85176405e-01  1.89855033e-01  2.25680719e-01  2.49816797e-01\n  3.37444444e-01  2.01833529e-01 -4.03112418e-01 -3.34374575e-01\n -2.54866732e-01 -2.16801111e-01 -1.39963595e-01 -2.75905882e-02\n -9.05124183e-03 -2.71558170e-02 -3.80196078e-02 -6.85377778e-02\n -1.38144444e-01 -2.27207516e-01 -2.77310523e-01 -1.45072614e-01\n  5.05557255e-01  3.20534314e-01  3.35026209e-01  2.71597451e-01\n  2.59995490e-01  2.58314967e-01  2.63989216e-01  2.02601176e-01\n  2.00219608e-01  1.92552484e-01  2.43280523e-01  2.40419020e-01\n  3.48805098e-01 -4.44836601e-02 -4.21184444e-01 -3.53188758e-01\n -2.67184314e-01 -2.27107255e-01 -1.63295686e-01 -3.34662092e-02\n -1.05666013e-02 -3.08207190e-02 -4.27741830e-02 -7.57775817e-02\n -1.47275948e-01 -2.20213791e-01 -2.97562157e-01 -1.70309804e-01\n  4.29908105e-01  3.02708954e-01  3.06438039e-01  2.47993399e-01\n  2.39493268e-01  2.48037255e-01  2.56803333e-01  1.99978954e-01\n  1.94438758e-01  1.89094967e-01  2.35103791e-01  2.50846078e-01\n  3.49099869e-01 -1.14520523e-01 -4.30792745e-01 -3.52523268e-01\n -2.70540000e-01 -2.31323333e-01 -1.81573007e-01 -4.10290196e-02\n -1.43754902e-02 -4.46743137e-02 -6.06222876e-02 -9.89322222e-02\n -1.83121569e-01 -2.74882876e-01 -3.27210654e-01 -3.25081111e-01\n  3.61984379e-01  2.99109412e-01  2.59543007e-01  2.23195686e-01\n  2.18030850e-01  2.44897386e-01  2.54273725e-01  2.00433791e-01\n  1.93513791e-01  1.84843529e-01  2.00233137e-01  2.71945163e-01\n  3.59721895e-01 -1.66873725e-01 -4.31493791e-01 -3.49714902e-01\n -2.79317255e-01 -2.34056405e-01 -1.96158693e-01 -4.91878431e-02\n -2.46235948e-02 -7.24652941e-02 -8.84316993e-02 -1.28219542e-01\n -2.15785359e-01 -2.92484183e-01 -3.44707255e-01 -3.68765752e-01\n  3.26437516e-01  2.68884706e-01  2.18872222e-01  2.11430915e-01\n  2.08644967e-01  2.33784314e-01  2.53112614e-01  2.01764444e-01\n  1.94200588e-01  1.85115098e-01  1.67171961e-01  2.81127582e-01\n  3.67604641e-01 -1.80215490e-01 -4.34704510e-01 -3.56113137e-01\n -2.88669608e-01 -2.39979281e-01 -2.09131046e-01 -5.71667320e-02\n -3.58619608e-02 -9.16413725e-02 -1.07023725e-01 -1.49787974e-01\n -2.39781895e-01 -3.14228824e-01 -3.63369608e-01 -3.91670458e-01\n  2.95196667e-01  2.37669608e-01  1.92850719e-01  2.01652810e-01\n  2.01903464e-01  2.21827582e-01  2.51938235e-01  2.01196797e-01\n  1.88720065e-01  1.85584837e-01  1.58926536e-01  2.68860065e-01\n  3.83785294e-01 -1.82370458e-01 -4.32347582e-01 -3.62802092e-01\n -2.94101961e-01 -2.41686732e-01 -2.12249542e-01 -6.00363399e-02\n -3.69159477e-02 -1.00243660e-01 -1.20796863e-01 -1.66643464e-01\n -2.58403268e-01 -3.27721176e-01 -3.76694510e-01 -4.00078889e-01\n  3.87327778e-01  2.26111765e-01  1.94358627e-01  1.90632353e-01\n  2.01266863e-01  2.25798431e-01  2.51359542e-01  2.05307582e-01\n  1.90653399e-01  1.91196405e-01  1.80432680e-01  2.64604902e-01\n  4.13391176e-01 -1.48756928e-01 -4.15798431e-01 -3.51479542e-01\n -2.85324510e-01 -2.29417451e-01 -1.99716732e-01 -5.63823529e-02\n -3.01676471e-02 -9.76177778e-02 -1.26256471e-01 -1.73636667e-01\n -2.65089281e-01 -3.33428758e-01 -3.78185425e-01 -3.97559216e-01\n  3.72144967e-01  2.27018627e-01  2.23096209e-01  1.97598562e-01\n  2.14036536e-01  2.43105425e-01  2.68208824e-01  2.22294379e-01\n  2.10643137e-01  2.01314379e-01  2.23202614e-01  2.70332745e-01\n  4.34901307e-01 -1.07095490e-01 -3.99822092e-01 -3.42500000e-01\n -2.74363725e-01 -2.16654118e-01 -1.85544183e-01 -5.21488235e-02\n -2.69092810e-02 -8.94477778e-02 -1.19334837e-01 -1.69826405e-01\n -2.61688889e-01 -3.26196993e-01 -3.69784248e-01 -3.86404706e-01\n  3.46466993e-01  2.51082484e-01  2.20660915e-01  2.17827451e-01\n  2.17043725e-01  2.50315556e-01  2.92166340e-01  2.29744183e-01\n  2.21163137e-01  2.33466732e-01  2.60312484e-01  2.76699542e-01\n  4.56827516e-01 -2.29432680e-02 -3.83825621e-01 -3.28324248e-01\n -2.61531895e-01 -2.02282288e-01 -1.64489608e-01 -4.21787582e-02\n -2.60863399e-02 -8.41080392e-02 -1.11604575e-01 -1.60746013e-01\n -2.51458235e-01 -3.14636078e-01 -3.56500196e-01 -3.70221176e-01\n  3.50043268e-01  2.67689281e-01  2.37881176e-01  2.38430588e-01\n  2.34688105e-01  2.68466536e-01  3.21547190e-01  2.59031111e-01\n  2.50132941e-01  2.61280196e-01  3.00850327e-01  3.04639216e-01\n  4.73766928e-01  4.59514379e-02 -3.56900850e-01 -3.06760000e-01\n -2.36929869e-01 -1.75122026e-01 -1.32750588e-01 -3.23790850e-02\n -2.31898693e-02 -7.47670588e-02 -1.01447843e-01 -1.50614052e-01\n -2.39469608e-01 -3.00198235e-01 -3.37648954e-01 -3.46838889e-01\n  3.75038431e-01  3.02212745e-01  2.61175033e-01  2.55510327e-01\n  2.57421373e-01  2.90548170e-01  3.54059869e-01  2.93592157e-01\n  2.83651830e-01  2.94339020e-01  3.45183529e-01  3.35472810e-01\n  4.91293007e-01  1.19536732e-01 -3.28586275e-01 -2.79588954e-01\n -2.11469542e-01 -1.50072680e-01 -1.10000327e-01 -2.71755556e-02\n -1.86118301e-02 -5.95922222e-02 -8.45809150e-02 -1.32571176e-01\n -2.19082549e-01 -2.81392418e-01 -3.12722941e-01 -3.19213791e-01\n  3.97846144e-01  3.41043791e-01  2.96545294e-01  2.80494444e-01\n  2.79238235e-01  3.20471438e-01  3.84243660e-01  3.41609346e-01\n  3.22090784e-01  3.28669935e-01  3.87359085e-01  3.74165621e-01\n  5.09932680e-01  1.64979608e-01 -2.98667320e-01 -2.50876144e-01\n -1.85425163e-01 -1.24582941e-01 -8.61005229e-02 -1.98208497e-02\n -1.30311765e-02 -4.33498039e-02 -6.69809150e-02 -1.13398301e-01\n -1.96454314e-01 -2.52097908e-01 -2.82717778e-01 -2.86636928e-01\n  4.07001765e-01  3.80811242e-01  3.36434510e-01  3.13555752e-01\n  3.04495098e-01  3.57398889e-01  4.06916013e-01  3.87872026e-01\n  3.60255033e-01  3.63122810e-01  4.17370784e-01  4.08413922e-01\n  5.29611765e-01  2.11408627e-01 -2.68002092e-01 -2.25563922e-01\n -1.58563203e-01 -9.89819608e-02 -6.29984314e-02 -1.36248366e-02\n -7.54366013e-03 -2.73781046e-02 -4.89088235e-02 -9.30378431e-02\n -1.72615556e-01 -2.26343137e-01 -2.51988301e-01 -2.52281307e-01\n  4.44940196e-01  4.29477320e-01  3.73549346e-01  3.51204837e-01\n  3.49817582e-01  4.01586797e-01  4.49511503e-01  4.27505098e-01\n  4.01442157e-01  3.96181961e-01  4.46832614e-01  4.45798235e-01\n  5.60676405e-01  2.54575425e-01 -2.40820654e-01 -2.00369673e-01\n -1.34548105e-01 -7.66333987e-02 -4.41557516e-02 -9.08588235e-03\n -3.69496732e-03 -1.55392157e-02 -3.33176471e-02 -7.27495425e-02\n -1.48236993e-01 -2.04694575e-01 -2.21163595e-01 -2.14657778e-01\n  3.88921438e-01  4.78455490e-01  4.09801046e-01  3.74011373e-01\n  3.71610196e-01  4.28995817e-01  4.71634967e-01  4.46201569e-01\n  4.17178170e-01  4.12340392e-01  4.65733660e-01  4.84338954e-01\n  5.91314379e-01  2.96797059e-01 -2.13524771e-01 -1.77157255e-01\n -1.08426928e-01 -5.40938562e-02 -2.76771242e-02 -5.28673203e-03\n -1.08993464e-03 -6.16823529e-03 -1.87308497e-02 -5.39100000e-02\n -1.24378039e-01 -1.78775948e-01 -1.92353203e-01 -1.80572876e-01\n  5.03485163e-01  5.66791438e-01  4.86473791e-01  5.31677908e-01\n  5.43176928e-01  5.88683333e-01  6.43846797e-01  6.22315229e-01\n  5.94163529e-01  5.86011503e-01  6.42490458e-01  5.94793595e-01\n  6.99316797e-01  4.62447582e-01 -1.87552092e-01 -1.43998889e-01\n -8.71020261e-02 -3.57489542e-02 -1.48366667e-02 -2.38633987e-03\n -9.15032680e-05 -8.33986928e-04 -3.89156863e-03 -1.70356863e-02\n -4.91890196e-02 -7.74798693e-02 -8.24131373e-02 -6.70140523e-02\n  4.58697647e-01  4.53124902e-01  3.46140850e-01  3.27241634e-01\n  3.23064771e-01  3.54958889e-01  3.22919020e-01  3.08977320e-01\n  2.93675033e-01  2.92266144e-01  3.31120131e-01  3.56044575e-01\n  4.44799020e-01  2.28109935e-01 -8.98778431e-02 -7.02623529e-02\n -3.32773203e-02 -1.06130719e-02 -3.21176471e-03 -2.77973856e-04].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "iaml01cw2_q1_6()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
